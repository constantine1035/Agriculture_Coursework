{
 "cells": [
  {
   "cell_type": "code",
   "id": "611ef821-953a-40d4-84b3-6c50d5e1a3dd",
   "metadata": {},
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "\n",
    "class AgricultureVisionDataset(Dataset):\n",
    "    def __init__(self, root_dir, split='train', transform=None):\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.images_dir = os.path.join(root_dir, split, \"images\", \"rgb\")\n",
    "        self.masks_dir  = os.path.join(root_dir, split, \"masks\")\n",
    "        \n",
    "        self.image_ids = sorted(os.listdir(self.images_dir))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image_filename = self.image_ids[idx]\n",
    "        image_path = os.path.join(self.images_dir, image_filename)\n",
    "        \n",
    "        mask_filename = os.path.splitext(image_filename)[0] + \".png\"\n",
    "        mask_path = os.path.join(self.masks_dir, mask_filename)\n",
    "        \n",
    "        image = np.array(Image.open(image_path).convert('RGB'))\n",
    "        mask  = np.array(Image.open(mask_path))\n",
    "        \n",
    "        if self.transform is not None:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask  = augmented['mask']\n",
    "        else:\n",
    "            image = ToTensorV2()(image=image)['image']\n",
    "            mask  = torch.as_tensor(mask, dtype=torch.long)\n",
    "        \n",
    "        mask = torch.as_tensor(mask, dtype=torch.long)\n",
    "        return image, mask\n",
    "\n",
    "def get_transforms(image_size=(256, 256)):\n",
    "    mean = [0.485, 0.456, 0.406]\n",
    "    std  = [0.229, 0.224, 0.225]\n",
    "    \n",
    "    train_transform = A.Compose([\n",
    "        A.Resize(*image_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, p=0.5),\n",
    "        A.Normalize(mean=mean, std=std),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    \n",
    "    val_transform = A.Compose([\n",
    "        A.Resize(*image_size),\n",
    "        A.Normalize(mean=mean, std=std),\n",
    "        ToTensorV2()\n",
    "    ])\n",
    "    return train_transform, val_transform\n",
    "\n",
    "def get_dataloaders(root_dir, batch_size=8, image_size=(256, 256), num_workers=0):\n",
    "    train_transform, val_transform = get_transforms(image_size=image_size)\n",
    "    \n",
    "    train_dataset = AgricultureVisionDataset(root_dir, split='train', transform=train_transform)\n",
    "    val_dataset   = AgricultureVisionDataset(root_dir, split='val', transform=val_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "    return train_loader, val_loader\n",
    "\n",
    "def create_model(model_name, encoder_name, num_classes, in_channels=3, encoder_weights=\"imagenet\"):\n",
    "    model = smp.create_model(\n",
    "        arch=model_name,\n",
    "        encoder_name=encoder_name,\n",
    "        encoder_weights=encoder_weights,\n",
    "        in_channels=in_channels,\n",
    "        classes=num_classes\n",
    "    )\n",
    "    return model\n",
    "\n",
    "def compute_confusion_matrix_and_metrics(model, data_loader, device, num_classes):\n",
    "    conf_matrix = np.zeros((num_classes, num_classes), dtype=np.int64)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(data_loader, desc=\"Computing metrics\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)  # [B, H, W]\n",
    "            outputs = model(images)   # [B, num_classes, H, W]\n",
    "            preds = torch.argmax(outputs, dim=1)  # [B, H, W]\n",
    "            \n",
    "            preds_flat = preds.view(-1).cpu().numpy()\n",
    "            masks_flat = masks.view(-1).cpu().numpy()\n",
    "            \n",
    "            indices = masks_flat * num_classes + preds_flat\n",
    "            bincount = np.bincount(indices, minlength=num_classes*num_classes)\n",
    "            conf_matrix += bincount.reshape((num_classes, num_classes))\n",
    "    \n",
    "    per_class_precision = []\n",
    "    per_class_recall = []\n",
    "    per_class_f1 = []\n",
    "    for c in range(num_classes):\n",
    "        tp = conf_matrix[c, c]\n",
    "        fp = conf_matrix[:, c].sum() - tp\n",
    "        fn = conf_matrix[c, :].sum() - tp\n",
    "        precision_c = tp / (tp + fp + 1e-7)\n",
    "        recall_c = tp / (tp + fn + 1e-7)\n",
    "        f1_c = 2 * precision_c * recall_c / (precision_c + recall_c + 1e-7)\n",
    "        per_class_precision.append(precision_c)\n",
    "        per_class_recall.append(recall_c)\n",
    "        per_class_f1.append(f1_c)\n",
    "    \n",
    "    avg_precision = np.mean(per_class_precision)\n",
    "    avg_recall = np.mean(per_class_recall)\n",
    "    avg_f1 = np.mean(per_class_f1)\n",
    "    \n",
    "    return conf_matrix, avg_precision, avg_recall, avg_f1\n",
    "\n",
    "\n",
    "def evaluate_epoch_metrics(model, data_loader, criterion, device, num_classes):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    total_pixels = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(data_loader, desc=\"Evaluating epoch\"):\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            batch_pixels = images.size(0) * images.size(2) * images.size(3)\n",
    "            running_loss += loss.item() * batch_pixels\n",
    "            total_pixels += batch_pixels\n",
    "    \n",
    "    seg_loss = running_loss / (total_pixels + 1e-7)\n",
    "    conf_mat, precision, recall, f1 = compute_confusion_matrix_and_metrics(model, data_loader, device, num_classes)\n",
    "    \n",
    "    return {\n",
    "        \"seg_loss\": seg_loss,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"conf_mat\": conf_mat\n",
    "    }\n",
    "\n",
    "def train_model(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    device, \n",
    "    num_classes, \n",
    "    num_epochs=25, \n",
    "    log_dir=\"logs\",\n",
    "    model_name=\"Unet\"\n",
    "):\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    best_val_loss = float('inf')\n",
    "    history = []\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        total_pixels = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch [{epoch+1}/{num_epochs}] - {model_name}\")\n",
    "        for images, masks in pbar:\n",
    "            images = images.to(device)\n",
    "            masks  = masks.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, masks)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            batch_pixels = images.size(0) * images.size(2) * images.size(3)\n",
    "            running_loss += loss.item() * batch_pixels\n",
    "            total_pixels += batch_pixels\n",
    "            \n",
    "            pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "        \n",
    "        train_seg_loss = running_loss / (total_pixels + 1e-7)\n",
    "        train_metrics = evaluate_epoch_metrics(model, train_loader, criterion, device, num_classes)\n",
    "        val_metrics   = evaluate_epoch_metrics(model, val_loader, criterion, device, num_classes)\n",
    "        \n",
    "        if val_metrics[\"seg_loss\"] < best_val_loss:\n",
    "            best_val_loss = val_metrics[\"seg_loss\"]\n",
    "            ckpt_path = os.path.join(log_dir, f\"best_{model_name}.pth\")\n",
    "            torch.save(model.state_dict(), ckpt_path)\n",
    "        \n",
    "        row = {\n",
    "            \"epoch\": epoch+1,\n",
    "            \"train_seg_loss\": train_metrics[\"seg_loss\"],\n",
    "            \"train_precision\": train_metrics[\"precision\"],\n",
    "            \"train_recall\": train_metrics[\"recall\"],\n",
    "            \"train_f1\": train_metrics[\"f1\"],\n",
    "            \"val_seg_loss\": val_metrics[\"seg_loss\"],\n",
    "            \"val_precision\": val_metrics[\"precision\"],\n",
    "            \"val_recall\": val_metrics[\"recall\"],\n",
    "            \"val_f1\": val_metrics[\"f1\"]\n",
    "        }\n",
    "        history.append(row)\n",
    "        \n",
    "        print(f\"[{model_name}] Epoch {epoch+1}/{num_epochs} | Train seg_loss={train_seg_loss:.4f}, Val seg_loss={val_metrics['seg_loss']:.4f}, Val F1={val_metrics['f1']:.4f}\")\n",
    "    \n",
    "    df = pd.DataFrame(history)\n",
    "    df_path = os.path.join(log_dir, f\"history_{model_name}.csv\")\n",
    "    df.to_csv(df_path, index=False)\n",
    "\n",
    "def visualize_predictions_and_save(model, data_loader, device, num_images=4, num_classes=9, save_dir=\"SMP\", model_name=\"model\"):\n",
    "    model.eval()\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    random.seed(42)\n",
    "    color_map = [ (random.randint(0,255), random.randint(0,255), random.randint(0,255)) for _ in range(num_classes)]\n",
    "    \n",
    "    images_list = []\n",
    "    preds_list = []\n",
    "    gt_list = []\n",
    "    count = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in data_loader:\n",
    "            for i in range(images.size(0)):\n",
    "                if count >= num_images:\n",
    "                    break\n",
    "                img_tensor = images[i].to(device).unsqueeze(0)\n",
    "                out = model(img_tensor)\n",
    "                pred = torch.argmax(out, dim=1).squeeze(0).cpu().numpy()\n",
    "                ground_truth = masks[i].cpu().numpy()\n",
    "                img_np = images[i].permute(1,2,0).cpu().numpy()\n",
    "                img_np = np.clip(img_np, 0, 1)\n",
    "                \n",
    "                images_list.append(img_np)\n",
    "                preds_list.append(pred)\n",
    "                gt_list.append(ground_truth)\n",
    "                count += 1\n",
    "            if count >= num_images:\n",
    "                break\n",
    "    \n",
    "    fig, axes = plt.subplots(num_images, 3, figsize=(15, 5*num_images))\n",
    "    if num_images == 1:\n",
    "        axes = np.expand_dims(axes, axis=0)\n",
    "    \n",
    "    for idx in range(num_images):\n",
    "        orig = images_list[idx]\n",
    "        pred = preds_list[idx]\n",
    "        gt_img = gt_list[idx]\n",
    "        \n",
    "        pred_color = np.zeros((pred.shape[0], pred.shape[1], 3), dtype=np.uint8)\n",
    "        gt_color   = np.zeros((gt_img.shape[0], gt_img.shape[1], 3), dtype=np.uint8)\n",
    "        \n",
    "        for c in range(num_classes):\n",
    "            pred_color[pred == c] = color_map[c]\n",
    "            gt_color[gt_img == c] = color_map[c]\n",
    "        \n",
    "        axes[idx, 0].imshow(orig)\n",
    "        axes[idx, 0].set_title(\"Original\")\n",
    "        axes[idx, 0].axis(\"off\")\n",
    "        \n",
    "        axes[idx, 1].imshow(orig, alpha=0.6)\n",
    "        axes[idx, 1].imshow(pred_color, alpha=0.4)\n",
    "        axes[idx, 1].set_title(\"Predicted Mask\")\n",
    "        axes[idx, 1].axis(\"off\")\n",
    "        \n",
    "        axes[idx, 2].imshow(orig, alpha=0.6)\n",
    "        axes[idx, 2].imshow(gt_color, alpha=0.4)\n",
    "        axes[idx, 2].set_title(\"Ground Truth\")\n",
    "        axes[idx, 2].axis(\"off\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    save_path = os.path.join(save_dir, f\"{model_name}_visualization.png\")\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"Saved visualization to {save_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    dataset_root = \"./AgricultureVision\"\n",
    "    num_classes  = 10\n",
    "    batch_size   = 4\n",
    "    num_epochs   = 5\n",
    "    image_size   = (512, 512)\n",
    "    device       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    log_dir      = \"smp_logs\"\n",
    "    \n",
    "    # Создаем DataLoader\n",
    "    train_loader, val_loader = get_dataloaders(\n",
    "        root_dir=dataset_root, \n",
    "        batch_size=batch_size, \n",
    "        image_size=image_size,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    model_configs = [\n",
    "        # Your configs\n",
    "    ]\n",
    "    \n",
    "    for (arch, encoder) in model_configs:\n",
    "        model_identifier = f\"{arch}_{encoder}\"\n",
    "        print(f\"\\n=== Training {model_identifier} ===\")\n",
    "        \n",
    "        model = create_model(\n",
    "            model_name=arch,\n",
    "            encoder_name=encoder,\n",
    "            num_classes=num_classes,\n",
    "            in_channels=3,\n",
    "            encoder_weights=\"imagenet\"\n",
    "        ).to(device)\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "        \n",
    "        train_model(\n",
    "            model=model,\n",
    "            train_loader=train_loader,\n",
    "            val_loader=val_loader,\n",
    "            criterion=criterion,\n",
    "            optimizer=optimizer,\n",
    "            device=device,\n",
    "            num_classes=num_classes,\n",
    "            num_epochs=num_epochs,\n",
    "            log_dir=log_dir,\n",
    "            model_name=model_identifier\n",
    "        )\n",
    "        \n",
    "        print(f\"=== Finished training {model_identifier} ===\")\n",
    "        ckpt_path = os.path.join(log_dir, f\"best_{model_identifier}.pth\")\n",
    "        model.load_state_dict(torch.load(ckpt_path))\n",
    "        \n",
    "        visualize_predictions_and_save(\n",
    "            model=model,\n",
    "            data_loader=val_loader,\n",
    "            device=device,\n",
    "            num_images=4,\n",
    "            num_classes=num_classes,\n",
    "            save_dir=\"SMP\",\n",
    "            model_name=model_identifier\n",
    "        )\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa496d9-e799-4442-81f9-1a6d9ab77dad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
